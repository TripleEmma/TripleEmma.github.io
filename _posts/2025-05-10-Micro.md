---
layout: post
date: 2025-05-10
title: Diving into Microbiome Data with QIIME 2 -- Lessons, Pitfalls, and Tricks
tag: Daily tasks
categories: [Bioinformatics]
---

I recently joined a project that involves reanalyzing microbiome sequencing data from published studies. For the analysis, we need the basic OTU or ASV tables derived from raw sequencing data. My task is to process the publicly available sequencing datasets and generate these tables when they are not provided directly by the original publications.
<!--more-->

In total, my colleagues sent me 23 publications. Nine of them indicated in their manuscripts that the scripts used to process the data are publicly available (e.g., on GitHub or Dryad). I’ve reviewed about half of these scripts so far, and I’ve learned a lot from them. However, I also noticed several issues and mistakes in some of them.

For example, one study provided raw sequencing data that had not been demultiplexed, meaning I needed barcode information to assign reads to samples. Their script clearly referenced a `barcode.fastq.gz` file, but this file wasn’t uploaded to NCBI. When I contacted the author to ask for the barcode file, she pointed me to a `meta.txt` file that listed which barcode corresponded to which sample. After several rounds of emails, I had the impression that the author didn’t fully understand what her script was actually doing. In the end, I had to abandon this dataset.

Another standout issue relates to training a primer-specific classifier. The study used the 515F/806R primer pair, targeting the 16S V4 region (~350 bp). It seems they intended to train a custom classifier, which is the correct approach. However, they downloaded the SILVA 138 reference files already trimmed to the 515F/806R region from QIIME2, and then re-applied the `qiime feature-classifier extract-reads` step using the same primers, which I believe is incorrect. Moreover, they set `--p-trunc-len 180`, which retains only the first 180 bp of the reference sequences. I assume they chose 180 because they had trimmed their sequencing reads to that length, but applying this to the reference sequences may cause a mismatch with the actual region. Overall, this gave me the impression that the authors didn’t fully understand the tools they were using.



#### Analysis tools

The most commonly used bioinformatics tool for microbiome analysis is **QIIME 2**, which also has a very well-maintained and active support forum. Another tool, **Mothur**, was used in two of the 23 studies I examined.

One of the first concepts I encountered when using QIIME 2 was the term “**artifact**”. At first, I was confused — I thought “artifact” referred to sequencing errors or false data. But in the context of QIIME 2, an artifact is actually a structured data object that contains both data and metadata. These are usually stored as `.qza` files. We can also covert the .qza file back to common data type like fastq and meta information table.

Another term I learned more deeply is “**amplicon**”. I had a vague idea of what it meant before — I thought it was similar to exome sequencing, just targeting a smaller region. But now I understand that amplicon sequencing involves amplifying a specific, small region (like the 16S V4) and sequencing it deeply. This helped me finally understand why NCBI requires information about the library construction type during data submission. Whether the library was generated through random fragmentation or targeted amplification (amplicon) makes a big difference for interpretation and processing. For example, one thing with amplicon sequencing that stood out to me is that PCR primers usually remain in the output reads. That was new to me. Though primers are generally not found in sequences when using the [EMP protocol](https://earthmicrobiome.org/protocols-and-standards/16s/), but  in most other library prep strategies, the primers are included in the sequences. These non-biological sequences (like primers, adapters, and linker pads) can interfere with OTU clustering and denoising (see below). Therefore, these sequences should be removed early in the workflow. In QIIME 2, this is usually done using the `cutadapt` plugin.


#### Data Quality
After removing the primers, the next step is to check the quality of the reads. This surprised me again. I had the impression that the base quality of Illumina reads is generally quite good — typically above Q25 even at the ends. But when I looked at the raw data from several of these microbiome studies, I was shocked: the quality of both forward and especially reverse reads dropped substantially toward the end.

One colleague who has worked with microbiome data for years told me that he sometimes discards reverse reads entirely because the quality is so poor. A common guideline is that if the 25th percentile quality score drops below Q25 at a certain position, the sequence should be truncated at that point. QIIME 2 makes it easy to visualize this using `qiime demux summarize`.

I now feel that every study should clearly report their read trimming and truncation settings, such as the exact positions used for forward and reverse reads. Unfortunately, this level of detail is often missing unless the authors share their full scripts or pipelines.

On a related note, I’ve started to realize that you can often tell — just by reading the methods or scripts — whether the authors really understand the bioinformatics pipeline or are just following a template without fully grasping each step.


#### Merging

If paired-end sequencing was used, the next step is to merge the forward and reverse reads by aligning their overlapping regions. This step is closely connected to the previous one — quality trimming — because the overlap occurs between the end of the forward read and the start of the reverse-complemented reverse read (that is what the end end of R2 we see).

If the read quality at the ends is too poor and too many bases are trimmed, the remaining sequences may no longer overlap sufficiently to be merged successfully.

The basic principle can be expressed as:
$$
\text{length of trimmed R1} + \text{length of trimmed R2} - \text{expected amplicon length} \geq \text{minimum required overlap}
$$

In QIIME 2, read merging is typically performed by DADA2, which by default requires at least 12 base pairs of overlap. If the trimmed sequences fall short of this requirement, the reads will fail to merge and will be discarded.


#### Denosing
DADA2 doesn’t just merge paired-end reads — it also performs denoising, which is a critical step in generating amplicon sequence variants (**ASVs**).

I’ve known the term **OTU** (Operational Taxonomic Unit) for nearly a decade, though I never deeply explored what it actually meant. About four years ago, I started seeing the term ASV more frequently. At the time, I understood ASVs as simply a higher-resolution version of OTUs — and that turns out to be fairly accurate.

Now I know that ASVs represent exact biological sequences, made possible by advances in sequencing technology (I doubt this, because the quality is not so good as I expected) and error-correction algorithms like those used in DADA2. While OTUs group sequences based on a similarity threshold (often 97%), ASVs allow for single-nucleotide resolution, providing a much finer view of microbial diversity.

Interestingly, this reminds me of GATK’s base quality score recalibration: both involve modeling and correcting sequencing errors to improve downstream results. Similarly, DADA2 constructs an error model from the data and uses it to distinguish between real biological variants and sequencing noise.

One of the 23 studies I examined used Sanger sequencing to obtain full-length 16S sequences. I asked ChatGPT whether forward and reverse Sanger reads could be processed using DADA2, and it replied that DADA2 is not designed for Sanger data. This seems plausible — like GATK’s base recalibration, DADA2 is built specifically for Illumina data, which has a very different error profile. That said, I haven’t verified this with the DADA2 documentation, so I’m not completely sure.

When working with single-end reads, the denoising is often done using Deblur instead of DADA2. Deblur relies on a static error model and doesn’t require paired-end information, making it more suitable for some single-end datasets.

Although ASVs are becoming increasingly common, QIIME 2 still supports OTU clustering. For example, in one of the 23 studies, I used vsearch, which is wrapped within QIIME 2, to reproduce the OTU table. 


#### Taxonomy assignment
The next step in my workflow was to assign taxonomy to each feature. In QIIME 2, a “feature” typically refers to either an ASV or an OTU.

At first, I assumed this process was similar to running a BLAST search — simply finding the closest match in a reference database. But QIIME 2 uses a more advanced approach. Instead of simple alignment, it employs a multinomial naive Bayes classifier implemented via [scikit-learn](http://scikit-learn.org/). This classifier is trained on a reference database (like SILVA or Greengenes) and then used to predict the taxonomy of each ASV based on its k-mer composition.

In the end, it still relies on sequence similarity, but it does so in a probabilistic and more robust way than direct alignment.

One of the most important lessons I’ve learned is to always use a classifier trained on the exact region amplified by your primers. In one study, the researchers included both positive and negative controls. The positive control was a ZymoBIOMICS mock community, which contains a known mixture of species. Ideally, I should be able to recover the expected genera from these samples.

When I used the generic full-length classifier (`silva-138-99-nb-classifier.qza`), more than half of the ASVs remained unassigned. However, after switching to a primer-specific classifier, I was able to detect 6 out of the 7 expected genera (the mock community only displayed 6 in the data I downloaded). Although the relative abundances were different from what was expected, the overall taxonomic assignment was much more accurate.


#### Filters

After taxonomy assignment, the next typical step is to filter the ASV (or OTU) table to remove unwanted features.

Almost every study applies a filter to remove features annotated as **mitochondria, chloroplast**, or **eukaryota**, since these are not part of the prokaryotic microbial community of interest. These reads often originate from host or plant organelles and can be safely discarded.

A more nuanced question is whether to remove features annotated as **archaea**. One of my colleagues pointed out that this decision depends heavily on the primers used for amplification. Some primer sets are **bacteria-specific**, while others can also amplify archaeal 16S regions. The SILVA TestPrime tool can be used to evaluate whether a given primer pair has coverage across archaeal sequences.

If the primers are not expected to amplify archaea, then archaeal reads likely represent noise and should be filtered out. Ultimately, the decision also depends on the biological question. If the goal is to study bacterial communities specifically, excluding archaeal features makes sense. But if the aim is to investigate total microbial diversity, including archaea, then they should be retained.

At this stage, it can also be helpful to apply additional filters — such as removing ASVs that appear in very few samples or with extremely low abundance (e.g. a total count <10 reads across all samples) — to reduce noise and avoid spurious diversity estimates. 

Apart from QIIME2 filter module, there are external R packages used to detected and remove potential contaminants.

- **`decontam`** is an R package designed to detect **contaminant sequences**, using patterns like:
  - Higher **prevalence in negative controls**, or
  - A **negative correlation with DNA concentration**.
     This statistical approach helps identify contaminants introduced during sample handling or library prep.


#### End

So far, these are the main steps I’ve completed. There are several downstream analyses — such as calculating **alpha** and **beta diversity**, building **phylogenetic trees**, and performing **ordination analyses** — but I haven’t delved into those yet. For now, the earlier steps have already provided the results I was looking for. I plan to explore the remaining analyses later when I have more time.



\
P.S. \
Another useful trick I learned is how to efficiently download all SRA files associated with a BioProject on NCBI. First, go to the [SRA Run Selector](https://www.ncbi.nlm.nih.gov/Traces/study/) and enter the BioProject ID to retrieve metadata for all related samples. From there, you can export the table, which includes all the **SRR accession numbers**. These SRR numbers can then be used with `fasterq-dump` from the **SRA Toolkit** to download the sequencing data in batch.

